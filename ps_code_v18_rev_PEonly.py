#Import all libraries of interest
import gzip 
import logging
import os
import re
import sys
import time
import uuid
import random
import numpy as np
import pandas as pd
import csv
from Bio import AlignIO, SeqIO
from Bio.Seq import Seq
from Bio.SeqRecord import SeqRecord
from Bio.Emboss.Applications import NeedleCommandline
import sys, getopt, os
#Code subset up through filter_pe_mismatch only

def load_ngs_file(fpath,ftype='fastq'):
    """
    Load a .fastq file to a SeqIO iterator, un gzip if necessary, as files are
    often returned to us as .gzip files.
    
    inputs:
        fpath - str, path to the file (either gzip or fastq)
        ftype = str, in this case always 'fastq' since NGS files are all FASTQ
        
    outputs:
        f_iter - generator object that doesn't actually load the data until you
        call upon it - saves time until you actually need the data
    """
    if fpath.endswith('.gz'):
        seq_f = gzip.open(fpath,'rb')
        # The above line opens a gzip-compressed file in binary or text mode
        # the 'rt' indicates it is opened in text mode
    elif fpath.endswith('.fastq'):
        seq_f = open(fpath,'rb')
        # If it finds any fastq files, it simply opens them in text format
    else:
        raise ValueError('File does not end in .gz or .fastq; confirm file type')
    f_iter = SeqIO.parse(seq_f,ftype)
    return f_iter
    
    #We also need a regex function (efficient ways of searching for string) for compiling 
    #This is essentially faster since we will compile the sequences first
    
def compile_res(seqs):
            """
            Compile regex for each string in a list, return list of regex objects
            
            input:
                seqs - list of sequences you wish to filter for, e.g. CS2, transposon scar
            """
            # Takes a list of sequences you want to filter for
            # Outputs a list of regex objects that you can iterate over
            # the list comprehension below, s stands for sequences in the seqs list
            return [re.compile(s) for s in seqs]
            
#Next we want to filter through all of the fastq file to only get the sequences
#that have the corresponding required sequences in f_res
            
def filter_seqs(seqs,q_re):
    """
    Filter an iterator based on whether items match a regex object, in this case the
    sequence itself.
    inputs:
        seqs: generator object
        q_re: a regex object generated by re.compile()
    outputs:
            list of Seq objects that have sequence in them.
    """

    out_l = [s for s in seqs if q_re.search(str(s.seq))]        
            #so here we compile a list of sequences from the fastq file, stripping away
            #the other data            
            
            #List comprehension for each line in seqs??
            #text_logger.info('Finished regex filter. Kept %i sequences.',len(out_l))
    return out_l
            
            #Next we will start filtering based on the sequences desired in compile_res

def quality_filter(seqs,q_cutoff=20):
            """
            removes sequences that have any bases below the cutoff score specified
            
            inputs:
                seqs
                q_cutoff - int, score below which a sequence will be removed
                
            outputs:
                out_l = list of sequences that survived the filtering
            """
            out_l = [s for s in seqs
                    if not any(s.letter_annotations['phred_quality'] < np.ones(len(s.letter_annotations['phred_quality']))*q_cutoff)]
            return out_l

def quality_filter_single(seqs,q_cutoff=20):
            """
            removes sequences that have any bases below the cutoff score specified
            
            inputs:
                seqs
                q_cutoff - int, score below which a sequence will be removed
                
            outputs:
                out_l = list of sequences that survived the filtering
            """
            out_l = 0
            if not any(seqs.letter_annotations['phred_quality'] < np.ones(len(seqs.letter_annotations['phred_quality']))*q_cutoff):
                out_l += 1
            return out_l

def filter_pe_mismatch(f_seqs,pe_seqs,copied_func,filt_seq): #Now edited to use the Needleman-Wunsch algorithm for paired-end filtering.
    """
    
    Inputs:
        f_seqs - list of sequences that survived filtering so far (SeqRecord objects)      
        pe_seqs - list of paired end sequences that survived filtering so far        
        copied_func - list of sequences; output of gen_copied_seq_function(f_res)
        
    Output:
        matched_seq_list - list of sequences that had paired end matches
    """
    #initialize variables
    matched_seq_list = []
    f_list = []
    pe_list = []
    missing_filt_seq = 0 #number of forward reads missing the back filter sequence
    missing_pe_filt_seq = 0 #number of PE reads missing the scar (shouldn't happen?)
    too_small_chunk = 0 #number of forward reads that had too small of a chunk to be kept
    bad_quality_reads = 0 #number of paired-end reads whose region inbetween alignment and scar has too low of a quality score to pass
    # bad_quality_reads_later = 0 #number of reads that fail the quality test after appending
    attempt_append = 0
    mismatched_len = 0 #number of reads that search the read and its complement badly
    read_len_list = [] #list of read lengths regardless of whether or not they pass the alignment score filter
    co_ct = 0 #number of sequences with coordinate matches
    aln_ct = 0 #number of sequences with paired end sequence matches
    append_ct = 0 #number of sequences that got appended
    #get coordinate list in the paired end reads
    count_list = []
    pe_coordL = [get_coords(s) for s in pe_seqs]
    pe_dict = {p.description:p for p in pe_seqs}
    #pe_coord_dict = {pe_coordL[s]:pe_dict}
    print('begin f_seqs loop:', len(f_seqs))

    si = 0

    for s in f_seqs:
            if pe_coordL.count(get_coords(s)):
                #Apparently the above line returns a boolean so long as the count
                #isn't zero, so if the paired-end coordinates were found, the block below will be run
                co_ct += 1 
                p_index = pe_coordL.index(get_coords(s))
                pe_read = pe_seqs[p_index].reverse_complement()
                #next set of conditionals ensures the forward and paired-end reads are the same length and shaves off bases accordingly if necessary
                if len(s) > len(pe_read):
                    s = s[0:len(pe_read)]
                elif len(s) < len(pe_read):
                    pe_seqs[p_index] = pe_seqs[p_index][0:len(s)]  
                    pe_read = pe_seqs[p_index].reverse_complement()     
                if filt_seq in str(s.seq): #if the scar is present in the forward read, proceed as with the perfect match
                    copied = copied_func(s)
                    with open('temp_seq_PE.fa','w') as sh: #create temporary seq file, hopefully re-written each time
                        SeqIO.write(copied,sh,'fastq')  
                    with open('temp_temp_PE.fa','w') as PE_seq_file:
                        SeqIO.write(pe_read,PE_seq_file,'fasta')

                    needle_cline = NeedleCommandline(cmd='needle',asequence='temp_seq_PE.fa', bsequence='temp_temp_PE.fa', gapopen=10,
                                                     gapextend=0.5, outfile='PE_copied.needle') #hopefully only one needle file gets made
                    needle_cline()
                    aln_data = list(AlignIO.parse(open('PE_copied.needle'),"emboss"))
                    bin_scores = [[46,251],[213,501],[458,751],[703,1001],[952,1251],[1128,1500],[1400,1750],[1650,2000],[1800,2250],[2150,2500]] #same bin cutoff scores as alignment
                    #initialize cutoff scores
                    lo_cutoff = 0
                    hi_cutoff = 0
                    #regex searches
                    # cond_bar = re.search('[AGCT]+',str(aln_data[0][0].seq)) #search forwards through forward (copied) read, find first base that aligned.
                    # cond_bar_f = re.search('[AGCT]+',str(aln_data[0][0].seq)[-1:0:-1]) #search backwards through forward (copied) read, find first base that aligned
                    # cond_match_coord_start =cond_bar.span()[0] #coordinates start from the first base of the forward read that aligned with the paired end read
                    # cond_match_coord_end = len(aln_data[0][0].seq)-cond_bar_f.span()[0] #coordinates end at the first base in reverse from the forward read
                    # cond_search_oligo = str(aln_data[0][0].seq)[cond_match_coord_start:cond_match_coord_end] #search oligo is the matching part from the forward (copied) read
                    scores = score_cutoff_by_length(str(aln_data[0][0].seq).strip('-'),bin_scores)
                    lo_cutoff = scores[0]
                    hi_cutoff = scores[1]
                    match_coord_start = 0
                    match_coord_end = 0 
                    match_len = 0
                    missing_align = 0
                    nonphys_overlap = 0
                    f = 0
                    if (aln_data[0].annotations['score'] >= lo_cutoff) and (aln_data[0].annotations['score'] <= hi_cutoff):
                        matched_seq_list.append(copied)
                        aln_ct += 1
                    else:
                        continue
                else: #if the scar isn't present in the forward read, start an appending process
                    with open('temp_seq_PE.fa','w') as sh: #create temporary seq file, hopefully re-written each time
                    #temp_f_seq = copied
                        SeqIO.write(s,sh,'fastq')  
                    with open('temp_temp_PE.fa','w') as PE_seq_file:
                        SeqIO.write(pe_read,PE_seq_file,'fasta')

                    needle_cline = NeedleCommandline(asequence='temp_seq_PE.fa', bsequence='temp_temp_PE.fa', gapopen=10,
                                                     gapextend=0.5, outfile='PE.needle') #hopefully only one needle file gets made
                    needle_cline()
                    aln_data = list(AlignIO.parse(open('PE.needle'),"emboss"))
                    bin_scores = [[46,251],[213,501],[458,751],[703,1001],[952,1251],[1128,1500],[1400,1750],[1650,2000],[1800,2250],[2150,2500]] #same bin cutoff scores as alignment
                    #initialize cutoff scores
                    lo_cutoff = 0
                    hi_cutoff = 1500
                    #if the scar isn't found on the forward read
                    missing_filt_seq +=1
                    bar = re.search('[AGCT]+',str(aln_data[0][1].seq)) #search forwards through reverse complement of PE read, find first base that aligned. 
                    bar_pe_r = re.search('[AGCT]+',str(aln_data[0][1].seq)[-1:0:-1]) #search backwards through PE read, find first base that aligned
                    bar_f = re.search('[AGCT]+',str(aln_data[0][0].seq))
                    bar_f_r = re.search('[AGCT]+',str(aln_data[0][0].seq)[-1:0:-1]) # search backwards through fwd read to find the first base that aligned
                    # if bar.span()[0] < bar_f.span()[0]: #this is if the fwd strand search goes longer until it hits an aligned base
                    match_coord_start = max([bar.span()[0],bar_f.span()[0]]) #coordinates start from the first base of the forward read that aligned with the paired end read, sometimes the paired end read has bases earlier
                    match_coord_end = min([(len(aln_data[0][0].seq)-bar_f_r.span()[0]),(len(aln_data[0][0].seq)-bar_pe_r.span()[0])])# coordinates end at the last aligned base in the forward read such that no bases present on the PE read but not fwd make it
                    search_oligo = str(aln_data[0][0].seq)[match_coord_start:match_coord_end] #coordinates are currently still based off the alignment alone; search oligo is only on forward base now
                    scores = score_cutoff_by_length(search_oligo,bin_scores)
                    lo_cutoff = scores[0]
                    hi_cutoff = scores[1]
                    match_coord_start = 0
                    match_coord_end = 0 
                    match_len = 0
                    missing_align = 0
                    nonphys_overlap = 0
                    f = 0
                    if (aln_data[0].annotations['score'] >= lo_cutoff) and (aln_data[0].annotations['score'] <= hi_cutoff):
                        aln_ct += 1
                    else:
                        continue
                    if len(search_oligo) < 12: #arbitrary cutoff for this aligned region based on fact that a 12 bp sequence is the smallest unique sequence in MBP
                        too_small_chunk += 1
                        continue
                    # elif len(search_oligo) > 50: #sometimes the entire region aligns, so I truncate it to just 50 bases for higher chance of alignment in the event of a mismatch surviving score filtering
                    #     search_oligo = search_oligo[len(search_oligo)-50:]
                    # #print('search oligo is '+str(len(search_oligo))+' bases long')
                    # bar1 = re.search(search_oligo,str(s.seq)) #find the aligned region in the forward sequence
                    # bar3  = re.search(str(Seq(search_oligo).reverse_complement()),str(pe_seqs[p_index].seq)) #find the aligned region's reverse complement in the actual PE sequence
                    bar4 = re.search(str(Seq(filt_seq).reverse_complement()),str(pe_seqs[p_index].seq)) # find the filt sequence's reverse complement (in this case the scar) in the actual PE 
                    # if str(type(bar3)) == "<type 'NoneType'>" or str(type(bar1)) == "<type 'NoneType'>" : #in the event there was a mismatch in the search oligo, the regex search will fail. Skip this iteration for the time being
                    #     missing_align += 1
                    #     continue
                    if str(type(bar4)) == "<type 'NoneType'>":
                        missing_pe_filt_seq += 1
                        continue
                    # elif bar4.span()[1] > bar3.span()[0]: # if some alignment happens such that part of the transposon scar aligns, this is messy and not worth dealing with
                    #     nonphys_overlap += 1
                    #     continue
                    f = quality_filter_single(pe_seqs[p_index][bar4.span()[1]:(bar4.span()[1]+50)],q_cutoff=20)
                    if f > 0: #if the quality of bases between the end of the aligned region and the start of the scar is good#
                        attempt_append += 1
                        temp_phred = pe_seqs[p_index].letter_annotations.values()[0][bar4.span()[1]:(bar4.span()[1]+50)] #temporarily dump Phred quality scores into a list
                        pe_seqs[p_index].letter_annotations = {} #clear the letter annotations so that the sequence can be changed
                        pe_seqs[p_index].seq = pe_seqs[p_index].seq[bar4.span()[1]:(bar4.span()[1]+50)].reverse_complement() #return only the part of the paired-end read up from end of the scar to the end of the aligned region; reverse complement
                        pe_seqs[p_index].letter_annotations = {'phred_quality':temp_phred} #now put back the new phred quality score list. Doesn't matter that it's flipped because we are only looking if any socre is below 20
                        matched_seq_list.append(pe_seqs[p_index]) #now this is appending the reverse complement of the paired end read so it can still reverse search
                        append_ct += 1 
                    else:
                        bad_quality_reads+=1
                        continue


            print si, " ", format(si/float(len(f_seqs))*100.0, '.2f'),"% percent complete            \r",
            sys.stdout.flush()
            si = si + 1
 
    # read_len_list = [f_list,pe_list]
    print ("\n\done.")
 
    # read_len_list = [f_list,pe_list]
    print ("")
    
    count_list.extend([co_ct,aln_ct]) #keep track of number of seqs with coord and align matches\
    with open('PE_statistics.csv','w') as file1:
        # should result in rxn1_828_829_F_results.csv as output
        file1.write(str(missing_filt_seq)+' reads were missing the reverse primer')
        file1.write(str(copied_too_short)+ ' reads had too small of a copied region')
        file1.write(str(missing_align)+ ' reads did not have a perfect aligned region, probably a mismatch')
        file1.write(str(too_small_chunk)+ ' reads had too small of an aligned region')
        file1.write(str(full_align)+ ' forward reads matched all the way to the last base')
        file1.close()
    return matched_seq_list,read_len_list

# def get_score(ofilen):
#         #Manually obtain the score from the needle file to avoid biopython issues
#     aln_data_list = list(AlignIO.parse(open(ofilen),"emboss"))
#     # for some reason this generator stuff is not working for me so I just set it as a list
#     a = open(ofilen,'r')
#     a1 = a.readlines()
#     score_list = []
#     for s in a1:
#         if 'Score' in s:
#             score_list.append = s.rstrip().lstrip('# Score: ')
#     aln_data_list1 = 0*len(aln_data_list)
#     for s in aln_data_list:
#         s.annotations = score_list[aln_data_list.index(s)]
#         aln_data_list1.append(s)
#     return aln_data_list1

def score_cutoff_by_length(sequence,bin_scores):

    """
    Determines the size of the sequence and assigns a corresponding bin low and high cutoff score for the Needleman-Wunsch algorithm

    Inputs:
        sequence - str, the characters (DNA bases here) for which a score cutoff is determined
        bin_scores - list of lists of integers containing the corresponding scores

    Outputs:
        cutoff_scores - list of two integers, the low and high cutoff scores

    """
    #initialize variables
    lo_cutoff = 0
    hi_cutoff = 0
    # determine length and set scores
    if len(sequence.lstrip('-').strip('-')) < 50 :
        lo_cutoff = bin_scores[0][0]
        hi_cutoff = bin_scores[0][1]
    elif len(sequence.lstrip('-').strip('-')) >= 50 and len(sequence.lstrip('-').strip('-')) < 100:
        lo_cutoff = bin_scores[1][0]
        hi_cutoff = bin_scores[1][1]
    elif len(sequence.lstrip('-').strip('-')) >= 100 and len(sequence.lstrip('-').strip('-')) < 150:
        lo_cutoff = bin_scores[2][0]
        hi_cutoff = bin_scores[2][1]
    elif len(sequence.lstrip('-').strip('-')) >= 150 and len(sequence.lstrip('-').strip('-')) < 200:
        lo_cutoff = bin_scores[3][0]
        hi_cutoff = bin_scores[3][1]
    elif len(sequence.lstrip('-').strip('-')) >= 200 and len(sequence.lstrip('-').strip('-')) < 250:
        lo_cutoff = bin_scores[4][0]
        hi_cutoff = bin_scores[4][1]
    elif len(sequence.lstrip('-').strip('-')) >= 250 and len(sequence.lstrip('-').strip('-')) <= 301:
        lo_cutoff = bin_scores[5][0]
        hi_cutoff = bin_scores[5][1]
    elif (len(sequence.strip('-')) >= 300) and (len(sequence.strip('-')) < 350):
        lo_cutoff = bin_scores[6][0]
        hi_cutoff = bin_scores[6][1]
    elif (len(sequence.strip('-')) >= 350) and (len(sequence.strip('-')) < 400):
        lo_cutoff = bin_scores[7][0]
        hi_cutoff = bin_scores[7][1]
    elif (len(sequence.strip('-')) >= 400) and (len(sequence.strip('-')) < 450):
        lo_cutoff = bin_scores[8][0]
        hi_cutoff = bin_scores[8][1]
    elif (len(sequence.strip('-')) >= 450) and (len(sequence.strip('-')) < 500):
        lo_cutoff = bin_scores[9][0]
        hi_cutoff = bin_scores[9][1]
    else:
        print(str(len(sequence.lstrip('-').strip('-')))+' is the length of the problematic read')
        raise ValueError('Sequence is either too long or too short; it is '+str(len(sequence.lstrip('-').strip('-')))+ ' bases long')
    cutoff_scores = [lo_cutoff,hi_cutoff]
    return cutoff_scores

def get_coords(s):
            return ':'.join(s.description.split(' ')[0].split(':')[3:])
            
def get_sense(s):
            return s.description.split(' ')[1].split(':')[0]

def get_copied_seq(s,f_res):
            # Finds part of the template sequence by basically copying the read sequence inbetween barcode and common
            # sequence
            #return s[f_res[0].search(str(s.seq)).end():list(f_res[1].finditer(str(s.seq)))[-1].start()]
            return s[f_res[0].search(str(s.seq)).start():list(f_res[2].finditer(str(s.seq)))[-1].start()]            
            # Indices will vary! Must find the sequence in between the f_res sequences

def gen_copied_seq_function(f_res):
            #This does something similar to get_copied_seq but returns a function of the SeqRecord object
            # instead of a list. This is for speed then?
            return lambda s: get_copied_seq(s, f_res)
            # lambda s is just like f(s)
#This is the part that gets run
def filter_sample(f_name,pe_name,template,f_filt_seqs,r_filt_seqs):

        f_res = compile_res(f_filt_seqs)
        pe_res = compile_res(r_filt_seqs)

        # Load FASTQ files as generator
        f_seqs = load_ngs_file(f_name)
        pe_seqs = load_ngs_file(pe_name)
        # Create lists of SeqRecord objects from the generators
        #Sometimes, the NGS reads have ambiguous bases or improperly sequenced bases, simply labled 'N'. We cannot use these reads and so they are removed here
        f_seqs1 = [s for s in f_seqs if 'N' not in str(s.seq)]
        pe_seqs1 = [s for s in pe_seqs if 'N' not in str(s.seq)]
        print(str(len(f_seqs1))+' forward reads and '+str(len(pe_seqs1))+' paired end reads initially')  
        #Filter for quality
        # These sequences with "2" at the end will be filtered for the sequences
        # corresponding to the MBP primer, ZFP primer, and the transposon scar
        f_seqs2 = []
        pe_seqs2 = []
        for regex in f_res:
            f_seqs2.append(filter_seqs(f_seqs1,regex))
                
        # As for the forward reads, repeat for the paired-end reads
        for regex in pe_res:
            pe_seqs2.append(filter_seqs(pe_seqs1,regex))
        print('Forward reads:'+str(len(f_seqs2[0]))+',',str(len(f_seqs2[1]))+',',str(len(f_seqs2[2])))
        print('PE reads:'+str(len(pe_seqs2[0]))+',',str(len(pe_seqs2[1]))+',',str(len(pe_seqs2[2])))
        #At this point, both f_seqs2 and pe_seqs2 will be a list of lists.

        #Depending on the direction the code searches in, a particular sequence is required for the both the forward and paired end reads. 
        f_seqs3 = f_seqs2[0]
        # Repeat for paired-end reads
        pe_seqs3 = pe_seqs2[2]

        print(str(len(f_seqs3))+' forward reads have the sequence of interest (MBP forward primer)')
        print(str(len(pe_seqs3))+' paired-end reads have the sequence of interest (transposon scar)')
        # Now that only sequences containing the CS and the TR have been filtered for, respectively
        # the paired-end matching can occur

        s1 = filter_pe_mismatch(f_seqs3,pe_seqs3,gen_copied_seq_function(f_res),f_filt_seqs[2]) #right now using the scar
        seqs = s1[0]
        with open('matched_seq_PE.fa','w') as sh: #create temporary seq file, hopefully re-written each time
                #temp_f_seq = copied
             SeqIO.write(seqs,sh,'fastq')
        read_len_postalign_list = s1[1]
        print(str(len(seqs))+' forward reads have a paired-end match')

        with open('read_lengths_PE.csv','w') as file:
            writer = csv.writer(file)
            writer.writerow(["pe_append","phred_len"])
            writer.writerows(s1[1])
            file.close()

        seqs = quality_filter(seqs,q_cutoff=20)
        print(str(len(seqs))+' forward reads survived the Phred score quality filter')

        return seqs



def main(argv):
    template_file = 'temptemplate.fa'
    template = str(list(SeqIO.parse(template_file,'fasta'))[0].seq)
    f_filt_seqs_file1 = ''
    f_name = ''
    pe_name = ''
    outp_name = ''

    try:
        opts, args = getopt.getopt(argv,"hc:f:p:o:",["csvfile=","ffile=","pefile=", "outpfile="])
    except getopt.GetoptError:
        print ('process_fileset.py -c <csvfile> -f <f_file> -p <pe_file> -o <out_file>')
        sys.exit(2)
    print(opts)
    for opt, arg in opts:
        if opt == '-h':
            print ('process_fileset.py -c <csvfile> -f <f_file> -p <pe_file> -o <out_file>')
            sys.exit()
        elif opt in ("-c", "--csvfile"):
            f_filt_seqs_file1 = arg
        elif opt in ("-f", "--ffile"):
            f_name = arg
        elif opt in ("-p", "--pefile"):
            pe_name = arg
        elif opt in ("-o", "--outpfile"):
            outp_name = arg

    print ('CSV File is"', f_filt_seqs_file1)
    print ('f_name file is "', f_name)
    print ('pe_name file is "', pe_name)
    print ('CSV output file is', outp_name)

    output_file_prefix = os.path.basename(os.path.splitext(f_filt_seqs_file1)[0])
    # this line strips something like './filter_sequences/rxn1_828_829_F.csv' to a simple string 'rxn1_828_829_F'
    # and that get used as a PREFIX of a file name for the figure file and result csv file below.

    f_df = pd.read_csv(f_filt_seqs_file1)
    f_filt_seqs = f_df['sequence'].tolist()
    print ('f_filt_seqs ', f_filt_seqs)
    # f_filt_seqs  ['CAACGATTCATACATAGCTAAAAGGTACC', 'CTCTAGGGCTAGCTCTAGCCAT', 'TGCGGCCGCA']
    r_filt_seqs = []
    #Generate reverse compliment for r_filt_seqs
    r_filt_seqs = [str(Seq(seq).reverse_complement()) for seq in f_filt_seqs]
    print ('r_filt_seqs ', r_filt_seqs)
    final_sequences = filter_sample(f_name,pe_name,template,f_filt_seqs,r_filt_seqs)
    print(str(len(final_sequences))+' forward reads survived the final filtering')

if __name__ == "__main__":
   main(sys.argv[1:])